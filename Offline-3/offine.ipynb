{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "        self.input = None  # To store the input for backward pass\n",
    "        self.d_weights = None  # To store gradients for weights\n",
    "        self.d_bias = None    # To store gradients for bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Save the input for the backward pass\n",
    "        self.input = X\n",
    "        # Compute the output\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        # Gradient of the loss with respect to the weights and bias\n",
    "        self.d_weights = np.dot(self.input.T, d_out)\n",
    "        self.d_bias = np.sum(d_out, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradient of the loss with respect to the input\n",
    "        d_input = np.dot(d_out, self.weights.T)\n",
    "\n",
    "        return d_input\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        # ReLU forward pass\n",
    "        self.input = X\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # Gradient for ReLU\n",
    "        return d_out * (self.input > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, dim, epsilon=1e-5, momentum=0.9):\n",
    "        self.gamma = np.ones(dim)\n",
    "        self.beta = np.zeros(dim)\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.running_mean = np.zeros(dim)\n",
    "        self.running_var = np.zeros(dim)\n",
    "        self.input = None  # To store the input for the backward pass\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            # Calculate batch statistics\n",
    "            self.mean = np.mean(X, axis=0)\n",
    "            self.var = np.var(X, axis=0)\n",
    "            self.input = X\n",
    "\n",
    "            # Normalize the input\n",
    "            self.X_norm = (X - self.mean) / np.sqrt(self.var + self.epsilon)\n",
    "            out = self.gamma * self.X_norm + self.beta\n",
    "\n",
    "            # Update running statistics\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "        else:\n",
    "            # Use running statistics for inference\n",
    "            X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            out = self.gamma * X_norm + self.beta\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        # Gradient computation for batch normalization\n",
    "        N, D = d_out.shape\n",
    "\n",
    "        # Intermediate values from forward pass\n",
    "        X_mu = self.input - self.mean\n",
    "        std_inv = 1. / np.sqrt(self.var + self.epsilon)\n",
    "\n",
    "        dX_norm = d_out * self.gamma\n",
    "        dvar = np.sum(dX_norm * X_mu, axis=0) * -0.5 * std_inv**3\n",
    "        dmean = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2. * X_mu, axis=0)\n",
    "\n",
    "        # Gradient w.r.t. input\n",
    "        dX = (dX_norm * std_inv) + (dvar * 2 * X_mu / N) + (dmean / N)\n",
    "        self.gamma -= learning_rate * np.sum(d_out * self.X_norm, axis=0)\n",
    "        self.beta -= learning_rate * np.sum(d_out, axis=0)\n",
    "        \n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            # Create dropout mask\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            # During inference, do nothing\n",
    "            return X\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # Apply dropout mask to gradient\n",
    "        return d_out * self.mask / (1 - self.dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}  # First moment vector\n",
    "        self.v = {}  # Second moment vector\n",
    "        self.t = 0   # Time step\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        self.t += 1\n",
    "        updated_params = {}\n",
    "\n",
    "        for key in params:\n",
    "            # Initialize moments if not already done\n",
    "            if key not in self.m:\n",
    "                self.m[key] = {'weights': np.zeros_like(grads[key]['weights']),\n",
    "                               'bias': np.zeros_like(grads[key]['bias'])}\n",
    "                self.v[key] = {'weights': np.zeros_like(grads[key]['weights']),\n",
    "                               'bias': np.zeros_like(grads[key]['bias'])}\n",
    "\n",
    "            # Update biased first moment estimate\n",
    "            self.m[key]['weights'] = self.beta1 * self.m[key]['weights'] + (1 - self.beta1) * grads[key]['weights']\n",
    "            self.m[key]['bias'] = self.beta1 * self.m[key]['bias'] + (1 - self.beta1) * grads[key]['bias']\n",
    "\n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[key]['weights'] = self.beta2 * self.v[key]['weights'] + (1 - self.beta2) * (grads[key]['weights'] ** 2)\n",
    "            self.v[key]['bias'] = self.beta2 * self.v[key]['bias'] + (1 - self.beta2) * (grads[key]['bias'] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first and second moment estimates\n",
    "            m_hat_weights = self.m[key]['weights'] / (1 - self.beta1 ** self.t)\n",
    "            m_hat_bias = self.m[key]['bias'] / (1 - self.beta1 ** self.t)\n",
    "            v_hat_weights = self.v[key]['weights'] / (1 - self.beta2 ** self.t)\n",
    "            v_hat_bias = self.v[key]['bias'] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update parameters\n",
    "            updated_params[key] = {\n",
    "                'weights': params[key]['weights'] - self.learning_rate * m_hat_weights / (np.sqrt(v_hat_weights) + self.epsilon),\n",
    "                'bias': params[key]['bias'] - self.learning_rate * m_hat_bias / (np.sqrt(v_hat_bias) + self.epsilon)\n",
    "            }\n",
    "\n",
    "        return updated_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, X):\n",
    "        exps = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return self.output * (d_out - np.sum(d_out * self.output, axis=1, keepdims=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork:\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate):\n",
    "        # Create a list of layers\n",
    "        self.layers = []\n",
    "        \n",
    "        # Add first Dense layer\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.layers.append(DenseLayer(prev_dim, hidden_dim))\n",
    "            self.layers.append(BatchNormalization(hidden_dim))\n",
    "            self.layers.append(ReLU())\n",
    "            self.layers.append(Dropout(dropout_rate))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Add final Dense layer\n",
    "        self.layers.append(DenseLayer(prev_dim, output_dim))\n",
    "        self.layers.append(Softmax())\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        # Forward pass through all layers\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                X = layer.forward(X, training)\n",
    "            elif isinstance(layer, BatchNormalization):\n",
    "                X = layer.forward(X, training)\n",
    "            else:\n",
    "                X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        # Backward pass through all layers in reverse order\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, (DenseLayer, BatchNormalization)):\n",
    "                d_out = layer.backward(d_out, learning_rate)\n",
    "            elif isinstance(layer, (ReLU, Dropout, Softmax)):\n",
    "                d_out = layer.backward(d_out)\n",
    "\n",
    "    def update_params(self, adam_optimizer):\n",
    "        # Prepare parameter and gradient dictionaries\n",
    "        params = {}\n",
    "        grads = {}\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, DenseLayer):\n",
    "                # Store weights and biases in params dictionary\n",
    "                params[id(layer)] = {'weights': layer.weights, 'bias': layer.bias}\n",
    "                \n",
    "                # Store gradients for weights and biases in grads dictionary\n",
    "                grads[id(layer)] = {'weights': layer.d_weights, 'bias': layer.d_bias}\n",
    "        \n",
    "        # Update parameters using Adam optimizer\n",
    "        updated_params = adam_optimizer.update(params, grads)\n",
    "\n",
    "        # Update the layers with the new parameters\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, DenseLayer):\n",
    "                layer.weights = updated_params[id(layer)]['weights']\n",
    "                layer.bias = updated_params[id(layer)]['bias']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Load the test dataset separately\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 65.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 632.4594, Accuracy: 0.8001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 67.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 427.4006, Accuracy: 0.8507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 51.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 391.8866, Accuracy: 0.8635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:19<00:00, 48.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 369.8415, Accuracy: 0.8692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:16<00:00, 58.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 354.7932, Accuracy: 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 67.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 341.9681, Accuracy: 0.8791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 63.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 335.0120, Accuracy: 0.8811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 64.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 327.1664, Accuracy: 0.8849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 65.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 319.0486, Accuracy: 0.8879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 65.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 313.0244, Accuracy: 0.8882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 28 * 28  # Image size (28x28)\n",
    "hidden_dims = [128, 64]  # Hidden layers\n",
    "output_dim = 10  # Number of classes (0-9)\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the model\n",
    "model = FeedForwardNeuralNetwork(input_dim, hidden_dims, output_dim, dropout_rate)\n",
    "adam_optimizer = AdamOptimizer(learning_rate)\n",
    "\n",
    "# DataLoader for training\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm.tqdm(train_loader):\n",
    "        # Flatten images\n",
    "        images = images.view(-1, 28 * 28).numpy()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model.forward(images, training=True)\n",
    "        \n",
    "        # One-hot encoding for labels\n",
    "        one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "\n",
    "        # Compute loss (cross-entropy loss)\n",
    "        loss = -np.sum(one_hot_labels * np.log(outputs + 1e-8)) / len(labels)\n",
    "        epoch_loss += loss\n",
    "\n",
    "        # Backward pass\n",
    "        d_out = outputs - np.eye(output_dim)[labels.numpy()]\n",
    "        model.backward(d_out, learning_rate)\n",
    "\n",
    "        # Update parameters\n",
    "        model.update_params(adam_optimizer)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = np.argmax(outputs, axis=1)\n",
    "        correct += (predictions == labels.numpy()).sum()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    # Display epoch results\n",
    "    epoch_accuracy = correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8741\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# DataLoader for testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Testing phase\n",
    "model_accuracy = 0\n",
    "model_total = 0\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    # Flatten images\n",
    "    images = images.view(-1, 28 * 28).numpy()\n",
    "\n",
    "    # Forward pass (inference mode)\n",
    "    outputs = model.forward(images, training=False)\n",
    "    predictions = np.argmax(outputs, axis=1)\n",
    "\n",
    "    # Collect results\n",
    "    all_predictions.extend(predictions)\n",
    "    all_true_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculate accuracy using sklearn\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model to a file\n",
    "with open('fashion_mnist_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the file\n",
    "with open('fashion_mnist_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.63%\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Run the model on the test data\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "import torch\n",
    "\n",
    "# Disable gradient computation for testing\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # Flatten the images\n",
    "        images = images.view(images.size(0), -1).numpy()\n",
    "\n",
    "        # Forward pass through the loaded model\n",
    "        outputs = loaded_model.forward(images)\n",
    "\n",
    "        # Get the predicted class (highest probability)\n",
    "        predictions = np.argmax(outputs, axis=1)\n",
    "\n",
    "        # Collect predictions and true labels for accuracy computation\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
