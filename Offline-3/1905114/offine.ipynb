{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "        self.input = None\n",
    "        self.d_weights = None\n",
    "        self.d_bias = None  \n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        self.d_weights = np.dot(self.input.T, d_out)\n",
    "        self.d_bias = np.sum(d_out, axis=0, keepdims=True)\n",
    "        d_input = np.dot(d_out, self.weights.T)\n",
    "        return d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return d_out * (self.input > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, dim, epsilon=1e-5, momentum=0.9):\n",
    "        self.gamma = np.ones(dim)\n",
    "        self.beta = np.zeros(dim)\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.running_mean = np.zeros(dim)\n",
    "        self.running_var = np.zeros(dim)\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.mean = np.mean(X, axis=0)\n",
    "            self.var = np.var(X, axis=0)\n",
    "            self.input = X\n",
    "\n",
    "            self.X_norm = (X - self.mean) / np.sqrt(self.var + self.epsilon)\n",
    "            out = self.gamma * self.X_norm + self.beta\n",
    "\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "        else:\n",
    "            X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            out = self.gamma * X_norm + self.beta\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        N, D = d_out.shape\n",
    "\n",
    "        X_mu = self.input - self.mean\n",
    "        std_inv = 1. / np.sqrt(self.var + self.epsilon)\n",
    "\n",
    "        dX_norm = d_out * self.gamma\n",
    "        dvar = np.sum(dX_norm * X_mu, axis=0) * -0.5 * std_inv**3\n",
    "        dmean = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2. * X_mu, axis=0)\n",
    "\n",
    "        dX = (dX_norm * std_inv) + (dvar * 2 * X_mu / N) + (dmean / N)\n",
    "        self.gamma -= learning_rate * np.sum(d_out * self.X_norm, axis=0)\n",
    "        self.beta -= learning_rate * np.sum(d_out, axis=0)\n",
    "        \n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return d_out * self.mask / (1 - self.dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        self.t += 1\n",
    "        updated_params = {}\n",
    "\n",
    "        for key in params:\n",
    "            if key not in self.m:\n",
    "                self.m[key] = [np.zeros_like(grads[key][0]),np.zeros_like(grads[key][1])]\n",
    "                self.v[key] = [np.zeros_like(grads[key][0]),np.zeros_like(grads[key][1])]\n",
    "\n",
    "            self.m[key][0] = self.beta1 * self.m[key][0] + (1 - self.beta1) * grads[key][0]\n",
    "            self.m[key][1] = self.beta1 * self.m[key][1] + (1 - self.beta1) * grads[key][1]\n",
    "\n",
    "            self.v[key][0] = self.beta2 * self.v[key][0] + (1 - self.beta2) * (grads[key][0] ** 2)\n",
    "            self.v[key][1] = self.beta2 * self.v[key][1] + (1 - self.beta2) * (grads[key][1] ** 2)\n",
    "\n",
    "            m_hat_weights = self.m[key][0] / (1 - self.beta1 ** self.t)\n",
    "            m_hat_bias = self.m[key][1] / (1 - self.beta1 ** self.t)\n",
    "            v_hat_weights = self.v[key][0] / (1 - self.beta2 ** self.t)\n",
    "            v_hat_bias = self.v[key][1] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            updated_params[key] =[params[key][0] - self.learning_rate * m_hat_weights / (np.sqrt(v_hat_weights) + self.epsilon),\n",
    "                                    params[key][1] - self.learning_rate * m_hat_bias / (np.sqrt(v_hat_bias) + self.epsilon)]\n",
    "\n",
    "        return updated_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, X):\n",
    "        exps = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return self.output * (d_out - np.sum(d_out * self.output, axis=1, keepdims=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork:\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate):\n",
    "        self.layers = []\n",
    "        \n",
    "        # Adding first Dense layer\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.layers.append(DenseLayer(prev_dim, hidden_dim))\n",
    "            self.layers.append(BatchNormalization(hidden_dim))\n",
    "            self.layers.append(ReLU())\n",
    "            self.layers.append(Dropout(dropout_rate))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Adding final Dense layer\n",
    "        self.layers.append(DenseLayer(prev_dim, output_dim))\n",
    "        self.layers.append(Softmax())\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        # Forward pass through all layers\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, (Dropout,BatchNormalization)):\n",
    "                X = layer.forward(X, training)\n",
    "            else:\n",
    "                X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        # Backward pass through all layers in reverse order\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, (DenseLayer, BatchNormalization)):\n",
    "                d_out = layer.backward(d_out, learning_rate)\n",
    "            elif isinstance(layer, (ReLU, Dropout, Softmax)):\n",
    "                d_out = layer.backward(d_out)\n",
    "\n",
    "    def update_params(self, adam_optimizer):\n",
    "        # Parameter and gradient dicts\n",
    "        params = {}\n",
    "        grads = {}\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, DenseLayer):\n",
    "                params[id(layer)] = [layer.weights,layer.bias]\n",
    "                grads[id(layer)] = [layer.d_weights,layer.d_bias]\n",
    "        \n",
    "        # Updating parameters using Adam optimizer\n",
    "        updated_params = adam_optimizer.update(params, grads)\n",
    "\n",
    "        # Updating the layers with the new parameters\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, DenseLayer):\n",
    "                layer.weights = updated_params[id(layer)][0]\n",
    "                layer.bias = updated_params[id(layer)][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Load the test dataset separately\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 70.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  60000 correct:  48517\n",
      "Epoch [1/10], Loss: 0.9563, Accuracy: 0.8086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 712/938 [00:12<00:03, 58.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 75\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfnn\n\u001b[0;32m     74\u001b[0m model1 \u001b[38;5;241m=\u001b[39m Model(input_dim,hidden_dims,output_dim,dropout_rate,learning_rate,batch_size,num_epochs)\n\u001b[1;32m---> 75\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[83], line 38\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m     35\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     36\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(train_loader):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Flatten images\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m28\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\datasets\\mnist.py:139\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[0;32m    132\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m        index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m        tuple: (image, target) where target is index of the target class.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "input_dim = 28 * 28  # Image size (28x28)\n",
    "hidden_dims = [128, 64]  # Hidden layers\n",
    "output_dim = 10  # Number of classes (0-9)\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# DataLoader for training\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,input_dim,hidden_dims,output_dim,dropout_rate,learning_rate,batch_size,num_epochs):\n",
    "        # Hyperparameters\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim \n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size \n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Initialize the fnn\n",
    "        self.fnn = FeedForwardNeuralNetwork(self.input_dim, self.hidden_dims, self.output_dim, self.dropout_rate)\n",
    "        self.adam_optimizer = AdamOptimizer(self.learning_rate)\n",
    "\n",
    "    def fit(self,train_loader):\n",
    "        # Training loop\n",
    "        print(self.num_epochs)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for images, labels in tqdm.tqdm(train_loader):\n",
    "                # Flatten images\n",
    "                images = images.view(-1, 28 * 28).numpy()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.fnn.forward(images, training=True)\n",
    "                \n",
    "                # One-hot encoding for labels\n",
    "                one_hot_labels = np.eye(self.output_dim)[labels.numpy()]\n",
    "\n",
    "                # Compute loss (cross-entropy loss)\n",
    "                loss = -np.sum(one_hot_labels * np.log(outputs + 1e-8)) / len(labels)\n",
    "                #print(len(labels))\n",
    "                epoch_loss += loss/(self.output_dim*self.batch_size)\n",
    "                #break\n",
    "                # Backward pass\n",
    "                d_out = outputs - np.eye(self.output_dim)[labels.numpy()]\n",
    "                self.fnn.backward(d_out, self.learning_rate)\n",
    "\n",
    "                # Update parameters\n",
    "                self.fnn.update_params(self.adam_optimizer)\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predictions = np.argmax(outputs, axis=1)\n",
    "                correct += (predictions == labels.numpy()).sum()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            # Display epoch results\n",
    "            print(\"total: \",total,\"correct: \",correct)\n",
    "            epoch_accuracy = correct / total\n",
    "            print(f'Epoch [{epoch+1}/{self.num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "        return self.fnn\n",
    "\n",
    "\n",
    "\n",
    "model1 = Model(input_dim,hidden_dims,output_dim,dropout_rate,learning_rate,batch_size,num_epochs)\n",
    "model = model1.fit(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8388\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# DataLoader for testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Testing phase\n",
    "model_accuracy = 0\n",
    "model_total = 0\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    # Flatten images\n",
    "    images = images.view(-1, 28 * 28).numpy()\n",
    "\n",
    "    # Forward pass (inference mode)\n",
    "    outputs = model.forward(images, training=False)\n",
    "    predictions = np.argmax(outputs, axis=1)\n",
    "\n",
    "    # Collect results\n",
    "    all_predictions.extend(predictions)\n",
    "    all_true_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculate accuracy using sklearn\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model to a file\n",
    "with open('fashion_mnist_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the file\n",
    "with open('fashion_mnist_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.36%\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Run the model on the test data\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "import torch\n",
    "\n",
    "# Disable gradient computation for testing\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # Flatten the images\n",
    "        images = images.view(images.size(0), -1).numpy()\n",
    "\n",
    "        # Forward pass through the loaded model\n",
    "        outputs = loaded_model.forward(images)\n",
    "\n",
    "        # Get the predicted class (highest probability)\n",
    "        predictions = np.argmax(outputs, axis=1)\n",
    "\n",
    "        # Collect predictions and true labels for accuracy computation\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
