{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "        self.input = None\n",
    "        self.d_weights = None\n",
    "        self.d_bias = None  \n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        self.d_weights = np.dot(self.input.T, d_out)\n",
    "        self.d_bias = np.sum(d_out, axis=0, keepdims=True)\n",
    "        d_input = np.dot(d_out, self.weights.T)\n",
    "        return d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return d_out * (self.input > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, dim, epsilon=1e-5, momentum=0.9):\n",
    "        self.gamma = np.ones(dim)\n",
    "        self.beta = np.zeros(dim)\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.running_mean = np.zeros(dim)\n",
    "        self.running_var = np.zeros(dim)\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.mean = np.mean(X, axis=0)\n",
    "            self.var = np.var(X, axis=0)\n",
    "            self.input = X\n",
    "\n",
    "            self.X_norm = (X - self.mean) / np.sqrt(self.var + self.epsilon)\n",
    "            out = self.gamma * self.X_norm + self.beta\n",
    "\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "        else:\n",
    "            X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            out = self.gamma * X_norm + self.beta\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        N, D = d_out.shape\n",
    "\n",
    "        X_mu = self.input - self.mean\n",
    "        std_inv = 1. / np.sqrt(self.var + self.epsilon)\n",
    "\n",
    "        dX_norm = d_out * self.gamma\n",
    "        dvar = np.sum(dX_norm * X_mu, axis=0) * -0.5 * std_inv**3\n",
    "        dmean = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2. * X_mu, axis=0)\n",
    "\n",
    "        dX = (dX_norm * std_inv) + (dvar * 2 * X_mu / N) + (dmean / N)\n",
    "        self.gamma -= learning_rate * np.sum(d_out * self.X_norm, axis=0)\n",
    "        self.beta -= learning_rate * np.sum(d_out, axis=0)\n",
    "        \n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return d_out * self.mask / (1 - self.dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        self.t += 1\n",
    "        updated_params = {}\n",
    "\n",
    "        for key in params:\n",
    "            if key not in self.m:\n",
    "                self.m[key] = [np.zeros_like(grads[key][0]),np.zeros_like(grads[key][1])]\n",
    "                self.v[key] = [np.zeros_like(grads[key][0]),np.zeros_like(grads[key][1])]\n",
    "\n",
    "            self.m[key][0] = self.beta1 * self.m[key][0] + (1 - self.beta1) * grads[key][0]\n",
    "            self.m[key][1] = self.beta1 * self.m[key][1] + (1 - self.beta1) * grads[key][1]\n",
    "\n",
    "            self.v[key][0] = self.beta2 * self.v[key][0] + (1 - self.beta2) * (grads[key][0] ** 2)\n",
    "            self.v[key][1] = self.beta2 * self.v[key][1] + (1 - self.beta2) * (grads[key][1] ** 2)\n",
    "\n",
    "            m_hat_weights = self.m[key][0] / (1 - self.beta1 ** self.t)\n",
    "            m_hat_bias = self.m[key][1] / (1 - self.beta1 ** self.t)\n",
    "            v_hat_weights = self.v[key][0] / (1 - self.beta2 ** self.t)\n",
    "            v_hat_bias = self.v[key][1] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            updated_params[key] =[params[key][0] - self.learning_rate * m_hat_weights / (np.sqrt(v_hat_weights) + self.epsilon),\n",
    "                                    params[key][1] - self.learning_rate * m_hat_bias / (np.sqrt(v_hat_bias) + self.epsilon)]\n",
    "\n",
    "        return updated_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, X):\n",
    "        exps = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return self.output * (d_out - np.sum(d_out * self.output, axis=1, keepdims=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork:\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate):\n",
    "        self.layers = []\n",
    "        \n",
    "        # Adding first Dense layer\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.layers.append(DenseLayer(prev_dim, hidden_dim))\n",
    "            self.layers.append(BatchNormalization(hidden_dim))\n",
    "            self.layers.append(ReLU())\n",
    "            self.layers.append(Dropout(dropout_rate))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Adding final Dense layer\n",
    "        self.layers.append(DenseLayer(prev_dim, output_dim))\n",
    "        self.layers.append(Softmax())\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        # Forward pass through all layers\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, (Dropout,BatchNormalization)):\n",
    "                X = layer.forward(X, training)\n",
    "            else:\n",
    "                X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        # Backward pass through all layers in reverse order\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, (DenseLayer, BatchNormalization)):\n",
    "                d_out = layer.backward(d_out, learning_rate)\n",
    "            elif isinstance(layer, (ReLU, Dropout, Softmax)):\n",
    "                d_out = layer.backward(d_out)\n",
    "\n",
    "    def update_params(self, adam_optimizer):\n",
    "        # Parameter and gradient dicts\n",
    "        params = {}\n",
    "        grads = {}\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, DenseLayer):\n",
    "                params[id(layer)] = [layer.weights,layer.bias]\n",
    "                grads[id(layer)] = [layer.d_weights,layer.d_bias]\n",
    "        \n",
    "        # Updating parameters using Adam optimizer\n",
    "        updated_params = adam_optimizer.update(params, grads)\n",
    "\n",
    "        # Updating the layers with the new parameters\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, DenseLayer):\n",
    "                layer.weights = updated_params[id(layer)][0]\n",
    "                layer.bias = updated_params[id(layer)][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Load the test dataset separately\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 67.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  60000 correct:  48574\n",
      "Epoch [1/10], Loss: 0.9496, Accuracy: 0.8096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 73.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  60000 correct:  51172\n",
      "Epoch [2/10], Loss: 0.6551, Accuracy: 0.8529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 72.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  60000 correct:  51852\n",
      "Epoch [3/10], Loss: 0.5994, Accuracy: 0.8642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 71.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  60000 correct:  52203\n",
      "Epoch [4/10], Loss: 0.5695, Accuracy: 0.8700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:17<00:00, 52.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  60000 correct:  52467\n",
      "Epoch [5/10], Loss: 0.5501, Accuracy: 0.8744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 69.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  60000 correct:  52682\n",
      "Epoch [6/10], Loss: 0.5347, Accuracy: 0.8780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 68.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  60000 correct:  52918\n",
      "Epoch [7/10], Loss: 0.5204, Accuracy: 0.8820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 68.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  60000 correct:  53130\n",
      "Epoch [8/10], Loss: 0.5103, Accuracy: 0.8855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 70.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  60000 correct:  53243\n",
      "Epoch [9/10], Loss: 0.4987, Accuracy: 0.8874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 69.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  60000 correct:  53331\n",
      "Epoch [10/10], Loss: 0.4879, Accuracy: 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 28 * 28  # Image size (28x28)\n",
    "hidden_dims = [128, 64]  # Hidden layers\n",
    "output_dim = 10  # Number of classes (0-9)\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the model\n",
    "model = FeedForwardNeuralNetwork(input_dim, hidden_dims, output_dim, dropout_rate)\n",
    "adam_optimizer = AdamOptimizer(learning_rate)\n",
    "\n",
    "# DataLoader for training\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_dataset))\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm.tqdm(train_loader):\n",
    "        # Flatten images\n",
    "        images = images.view(-1, 28 * 28).numpy()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model.forward(images, training=True)\n",
    "        \n",
    "        # One-hot encoding for labels\n",
    "        one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "\n",
    "        # Compute loss (cross-entropy loss)\n",
    "        loss = -np.sum(one_hot_labels * np.log(outputs + 1e-8)) / len(labels)\n",
    "        #print(len(labels))\n",
    "        epoch_loss += loss/(output_dim*batch_size)\n",
    "        #break\n",
    "        # Backward pass\n",
    "        d_out = outputs - np.eye(output_dim)[labels.numpy()]\n",
    "        model.backward(d_out, learning_rate)\n",
    "\n",
    "        # Update parameters\n",
    "        model.update_params(adam_optimizer)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = np.argmax(outputs, axis=1)\n",
    "        correct += (predictions == labels.numpy()).sum()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    # Display epoch results\n",
    "    print(\"total: \",total,\"correct: \",correct)\n",
    "    epoch_accuracy = correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8820\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# DataLoader for testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Testing phase\n",
    "model_accuracy = 0\n",
    "model_total = 0\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    # Flatten images\n",
    "    images = images.view(-1, 28 * 28).numpy()\n",
    "\n",
    "    # Forward pass (inference mode)\n",
    "    outputs = model.forward(images, training=False)\n",
    "    predictions = np.argmax(outputs, axis=1)\n",
    "\n",
    "    # Collect results\n",
    "    all_predictions.extend(predictions)\n",
    "    all_true_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculate accuracy using sklearn\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model to a file\n",
    "with open('fashion_mnist_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the file\n",
    "with open('fashion_mnist_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.58%\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Run the model on the test data\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "import torch\n",
    "\n",
    "# Disable gradient computation for testing\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # Flatten the images\n",
    "        images = images.view(images.size(0), -1).numpy()\n",
    "\n",
    "        # Forward pass through the loaded model\n",
    "        outputs = loaded_model.forward(images)\n",
    "\n",
    "        # Get the predicted class (highest probability)\n",
    "        predictions = np.argmax(outputs, axis=1)\n",
    "\n",
    "        # Collect predictions and true labels for accuracy computation\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
