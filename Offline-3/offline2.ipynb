{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Linear forward pass\n",
    "        self.input = X\n",
    "        self.output = np.dot(X, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        # Compute gradients for weights and bias\n",
    "        d_weights = np.dot(self.input.T, d_out)\n",
    "        d_bias = np.sum(d_out, axis=0, keepdims=True)\n",
    "        d_input = np.dot(d_out, self.weights.T)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * d_weights\n",
    "        self.bias -= learning_rate * d_bias\n",
    "\n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        # ReLU forward pass\n",
    "        self.input = X\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # Gradient for ReLU\n",
    "        return d_out * (self.input > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, dim, epsilon=1e-5, momentum=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = np.ones((1, dim))\n",
    "        self.beta = np.zeros((1, dim))\n",
    "        self.running_mean = np.zeros((1, dim))\n",
    "        self.running_var = np.zeros((1, dim))\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        self.input = X  # Store the input for use in backward pass\n",
    "        if training:\n",
    "            self.mean = np.mean(X, axis=0)\n",
    "            self.var = np.var(X, axis=0)\n",
    "            self.X_normalized = (X - self.mean) / np.sqrt(self.var + self.epsilon)\n",
    "            self.output = self.gamma * self.X_normalized + self.beta\n",
    "            # Update running mean and variance\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "        else:\n",
    "            # Use running mean and variance during inference\n",
    "            self.X_normalized = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            self.output = self.gamma * self.X_normalized + self.beta\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        # Gradient computation for batch normalization\n",
    "        N, D = d_out.shape\n",
    "        X_mu = self.input - self.mean\n",
    "        std_inv = 1. / np.sqrt(self.var + self.epsilon)\n",
    "\n",
    "        dX_norm = d_out * self.gamma\n",
    "        dvar = np.sum(dX_norm * X_mu, axis=0) * -0.5 * std_inv**3\n",
    "        dmean = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2. * X_mu, axis=0)\n",
    "\n",
    "        dX = (dX_norm * std_inv) + (dvar * 2 * X_mu / N) + (dmean / N)\n",
    "        dgamma = np.sum(d_out * self.X_normalized, axis=0)\n",
    "        dbeta = np.sum(d_out, axis=0)\n",
    "\n",
    "        # Update gamma and beta\n",
    "        self.gamma -= learning_rate * dgamma\n",
    "        self.beta -= learning_rate * dbeta\n",
    "\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            # Create dropout mask\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            # During inference, do nothing\n",
    "            return X\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # Apply dropout mask to gradient\n",
    "        return d_out * self.mask / (1 - self.dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        self.t += 1\n",
    "        updated_params = {}\n",
    "        for key in params:\n",
    "            if key not in self.m:\n",
    "                self.m[key] = np.zeros_like(grads[key])\n",
    "                self.v[key] = np.zeros_like(grads[key])\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key]**2)\n",
    "            # Correct bias in first moment\n",
    "            m_hat = self.m[key] / (1 - self.beta1**self.t)\n",
    "            # Correct bias in second raw moment\n",
    "            v_hat = self.v[key] / (1 - self.beta2**self.t)\n",
    "            # Update parameters\n",
    "            updated_params[key] = params[key] - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        \n",
    "        return updated_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, X):\n",
    "        exps = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return self.output * (d_out - np.sum(d_out * self.output, axis=1, keepdims=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.3476\n",
      "Epoch 2/10, Loss: 2.3708\n",
      "Epoch 3/10, Loss: 2.4276\n",
      "Epoch 4/10, Loss: 2.4475\n",
      "Epoch 5/10, Loss: 2.4427\n",
      "Epoch 6/10, Loss: 2.4643\n",
      "Epoch 7/10, Loss: 2.4825\n",
      "Epoch 8/10, Loss: 2.5228\n",
      "Epoch 9/10, Loss: 2.5561\n",
      "Epoch 10/10, Loss: 2.5473\n",
      "Validation Accuracy: 0.0030\n",
      "Test Accuracy: 0.0024\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Custom classes for layers and optimizer are assumed to be already defined as in the provided code\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 28 * 28  # Flatten 28x28 images\n",
    "hidden_dim = 128     # Number of neurons in hidden layer\n",
    "output_dim = 10      # Number of output classes (FashionMNIST has 10 classes)\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# 1. Load and preprocess the FashionMNIST dataset\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Split training dataset into train and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 2. Define the network architecture\n",
    "class FashionMNISTModel:\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            DenseLayer(input_dim, hidden_dim),\n",
    "            BatchNormalization(hidden_dim),\n",
    "            ReLU(),\n",
    "            Dropout(dropout_rate),\n",
    "            DenseLayer(hidden_dim, output_dim),\n",
    "            Softmax()\n",
    "        ]\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        # Sequentially apply each layer\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                X = layer.forward(X, training)\n",
    "            elif isinstance(layer, BatchNormalization):\n",
    "                X = layer.forward(X, training)\n",
    "            else:\n",
    "                X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        # Backward pass through each layer in reverse order\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, (DenseLayer, BatchNormalization)):\n",
    "                d_out = layer.backward(d_out, learning_rate)\n",
    "            else:\n",
    "                d_out = layer.backward(d_out)\n",
    "\n",
    "# 3. Training loop\n",
    "def train(model, train_loader, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Flatten the input data\n",
    "            data = data.view(data.size(0), -1).numpy()\n",
    "            target = target.numpy()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model.forward(data, training=True)\n",
    "\n",
    "            # Compute loss (cross-entropy)\n",
    "            loss = -np.sum(np.log(output[np.arange(len(target)), target])) / len(target)\n",
    "\n",
    "            # Backward pass\n",
    "            d_out = output\n",
    "            d_out[np.arange(len(target)), target] -= 1\n",
    "            d_out /= len(target)\n",
    "\n",
    "            # Update weights using optimizer\n",
    "            model.backward(d_out, optimizer.learning_rate)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "# 4. Evaluation function\n",
    "def evaluate(model, data_loader):\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    for data, target in data_loader:\n",
    "        data = data.view(data.size(0), -1).numpy()\n",
    "        target = target.numpy()\n",
    "\n",
    "        output = model.forward(data, training=False)\n",
    "        preds = np.argmax(output, axis=1)\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(target)\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    return accuracy\n",
    "\n",
    "# 5. Save model function\n",
    "def save_model(model, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# 6. Instantiate and train the model\n",
    "model = FashionMNISTModel()\n",
    "optimizer = AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "train(model, train_loader, optimizer, epochs)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = evaluate(model, val_loader)\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "save_model(model, 'fashion_mnist_model.pkl')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = evaluate(model, test_loader)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
